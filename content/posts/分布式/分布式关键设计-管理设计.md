---
title: "分布式关键设计 管理设计"
date: 2023-01-27T23:52:31+08:00
draft: false
tags: 
  - 分布式架构
  - 笔记
ShowToc: true
---

## 分布式锁

场景：
我们知道，在多线程情况下访问一些共享资源需要加锁，不然就会出现数据被写乱的问题。分布式环境下， 这样的问题也是一样的。只不过，我们需要一个分布式的锁服务。对于分布式的锁服务，一般可以用数据库 DB、Redis 和 ZooKeeper 等实现。

特点：
**安全性（Safety）**：在任意时刻，只有一个客户端可以获得锁（**排他性**）。
**避免死锁**：客户端最终一定可以获得锁，即使锁住某个资源的客户端在释放锁之前崩溃或者网络不可达。
**容错性**：只要锁服务集群中的大部分节点存活，Client 就可以进行加锁解锁操作。

设计：
我是用来修改某个共享源的，还是用来不同进程间的同步或是互斥的。如果使用 CAS 这样的方式（无锁方式）来更新数据，那么我们是不需要使用分布式锁服务的，而后者可能是需要的。
一般可以使用数据库、Redis 或 ZooKeeper 来做分布式锁服务。

1. 需要给一个锁被释放的方式，以避免请求者不把锁还回来，导致死锁的问题。 超时时间。
2. 分布式锁服务应该是高可用的，而且是需要持久化的。
3. 要提供非阻塞方式的锁服务。
4. 还要考虑锁的可重入性。

### 具体实现

**redis：**

```shell
SET resource_name my_random_value NX PX 30000

if redis.call("get",KEYS[1]) == ARGV[1] then
    return redis.call("del",KEYS[1])
else
    return 0
end
```

问题：
![Yhz6f4](http://qiniu.chalme.top/blog/20230127/Yhz6f4.jpg)
解决方案： 乐观锁，版本号递增；

1. 锁服务的时候，单调递增的版本号；
2. 写数据的时候也需要增加版本号；
3. 数据库保存版本号， 对情况进行校验；

**从乐观锁到 CAS**
数据库中也保留着版本号 ， 可以使用 乐观锁处理(**fence token**)；
![CQZ6kv](http://qiniu.chalme.top/blog/20230127/CQZ6kv.jpg)

```shell

UPDATE table_name SET xxx = #{xxx}, version=version+1 where version =#{version};
```

数据库那边一般会用 **timestamp** 时间截来玩。也是在更新提交的时候检查当前数据库中数据的时间戳和自己更新前取到的时间戳进行对比，如果一致则 OK，否则就是版本冲突。

有时候都**不需要增加额外的版本字段**或是 fence token。

```shell
SELECT stock FROM tb_product where product_id=#{product_id};
UPDATE tb_product SET stock=stock-#{num} WHERE product_id=#{product_id} AND stock=#{stock};
```

无锁结构： 乐观锁 CAS 。
cas 的 ABA 问题： double-CAS（双保险的 CAS）。
无锁队列：
1）无锁队列主要是通过 CAS、FAA 这些原子操作，和 Retry-Loop 实现。
2）对于 Retry-Loop，我个人感觉其实和锁什么什么两样。只是这种“锁”的粒度变小了，主要是“锁”HEAD 和 TAIL 这两个关键资源。而不是整个数据结构。

**trade-off**
悲观锁因为对读写都加锁，所以它的性能比较低，对于现在互联网提倡的三高(高性能、高可用、高并发)来说，悲观锁的实现用的越来越少了，但是一般多读的情况下还是需要使用悲观锁的，因为虽然加锁的性能比较低，但是也阻止了像乐观锁一样，遇到写不一致的情况下一直重试的时间。
相对而言，乐观锁用于读多写少的情况，即很少发生冲突的场景，这样可以省去锁的开销，增加系统的吞吐量。

## 管理设计

我们知道，除了代码之外，软件还有一些配置信息，比如数据库的用户名和密码，还有一些我们不想写死在代码里的东西，像线程池大小、队列长度等运行参数，以及日志级别、算法策略等，还有一些是软件运行环境的参数，如 Java 的内存大小，应用启动的参数，包括操作系统的一些参数配置……

### 配置中心的设计

把软件的配置分成静态配置和**动态配置**。操作系统的网络配置，软件运行时 Docker 进程的配置，这些配置在软件环境初始化时就确定了，未来基本不会修改了。而所谓动态配置其实就是软件运行时的一些配置，在运行时会被修改。比如，**日志级别、降级开关、活动开关**。
动态配置的管理，做好**区分， 三个部分：**

1. 按运行环境分。 开发，测试，生产。
2. 按依赖区分。一种是依赖配置，一种是不依赖的内部配置。比如，外部依赖的 MySQL 或 Redis 的连接配置。还有一种完全是自己内部的配置。
3. 按层次分。 分成 IaaS、PaaS、SaaS 三层。基础层的配置是操作系统的配置，中间平台层的配置是中间件的配置，如 Tomcat 的配置，上层软件层的配置是应用自己的配置。

配置中心的模型
软件配置基本上来说，每个配置项就是 key/value 的模型

配置中心的架构
![R2dqxK](http://qiniu.chalme.top/blog/20230127/R2dqxK.jpg)
在这个图中可以看到，我们把配置录入后，配置中心发出变更通知，配置变更控制器会来读取最新的配置，然后应用配置。
**为什么需要一个变更通知的组件**，而不是让配置中心直接推送？
**分布式环境下，服务器太多，推送不太现实**，而采用一个 Pub/Sub 的通知服务可以让数据交换经济一些。

配置中心的设计重点
配置中心主要的用处是统一和规范化管理所有的服务配置，也算是一种配置上的治理活动。所以，配置中心的设计重点应该放在如何统一和标准化软件的配置项，其还会涉及到软件版本、运行环境、平台、中间件等一系列的配置参数。

配置的实时：
写少读多的场景

1. 基于拉模型的客户端轮询的方案
1. 大多数轮询请求都是没有意义的
1. .基于推模型的客户端长轮询的方案
    1. 基于 Http 长轮询模型，实现了让客户端在没有发生动态配置变更的时候减少轮询。这样减少了无意义的轮询请求量，提高了轮询的效率；也降低了系统负载，提升了整个系统的资源利用率。

这种推拉结合的策略，做到了在长连接和短连接之间的平衡，实现上让服务端不用太关注连接的管理，效果上又获得了类似 TCP 长连接的信息推送的实时性。

## 边车模式

**编程的本质就是将控制和逻辑分离和解耦，而边车模式也是异曲同工，同样是让我们在分布式架构中做到逻辑和控制分离。**
所谓的边车模式，对应于我们生活中熟知的边三轮摩托车。也就是说，我们可以通过给一个摩托车加上一个边车的方式来扩展现有的服务和功能。这样可以很容易地做到 " 控制 " 和 " 逻辑 " 的分离。
也就是说，我们不需要在服务中实现控制面上的东西，如监视、日志记录、限流、熔断、服务注册、协议适配转换等这些属于控制面上的东西，而只需要专注地做好和业务逻辑相关的代码，然后，由“边车”来实现这些与业务逻辑没有关系的控制功能。

**对于监视、日志、限流、熔断、服务注册、协议转换等等这些功能**，其实都是大同小异，甚至是完全可以做成标准化的组件和模块的
一般来说，我们有两种方式。一种是通过 SDK、Lib 或 Framework 软件包方式，在开发时与真实的应用服务集成起来。
另一种是通过像 Sidecar 这样的方式，在运维时与真实的应用服务集成起来。

|      | 集成在软件                                                     | sidecar                                                                                              |
| ---- | -------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------- |
| 优点 | 以软件包的方式可以和应用密切集成，有利于资源的利用和应用的性能 | 对应用服务没有侵入性，并且不用受到应用服务的语言和技术的限制，而且可以做到控制和逻辑的分开升级和部署 |
| 缺点 | 对应用有侵入，而且受应用的编程语言和技术限制                   | 增加了每个应用服务的依赖性，也增加了应用的延迟，并且也会大大增加管理、托管、部署的复杂度。           |
|      |                                                                | Sidecar 可以帮助服务注册到相应的服务发现系统，并对服务做相关的健康检查。                             |

当应用服务要调用外部服务时， Sidecar 可以帮助从服务发现中找到相应外部服务的地址，然后做服务路由。
Sidecar 接管了进出的流量，我们就可以做相应的日志监视、调用链跟踪、流控熔断……这些都可以放在 Sidecar 里实现。然后，服务控制系统可以通过控制 Sidecar 来控制应用服务，如流控、下线等。 |

![Tk4xxm](http://qiniu.chalme.top/blog/20230127/Tk4xxm.jpg)

### 边车设计的重点

重点解决什么样的问题

1. 控制和逻辑的分离。
2. 服务调用中上下文的问题。

我们知道，**熔断、路由、服务发现、计量、流控、监视、重试、幂等、鉴权**等控制面上的功能，以及其相关的配置更新，本质来上来说，和服务的关系并不大。但是传统的工程做法是在开发层面完成这些功能，这就会导致各种维护上的问题，而且还会受到特定语言和编程框架的约束和限制。

设计的重点：

1. 进程间通讯机制是这个设计模式的重点
2. 服务协议方面，也请使用标准统一的方式。这里有两层协议，一个是 Sidecar 到 service 的内部协议，另一个是 Sidecar 到远端 Sidecar 或 service 的外部协议。
3. 使用这样的模式，需要在服务的整体打包、构建、部署、管控、运维上设计好。
4. 等等

适用的场景：

1. 把控制和逻辑分离，标准化控制面上的动作和技术，从而提高系统整体的稳定性和可用性。也有利于分工——并不是所有的程序员都可以做好控制面上的开发的。
2. 一个比较明显的场景是对老应用系统的改造和扩展。

不适用于什么样的场景：

1. 架构并不复杂的时候，不需要使用这个模式，直接使用 API Gateway 或者 Nginx 和 HAProxy 等即可。
2. 服务间的协议不标准且无法转换。
3. 不需要分布式的架构。

## 服务网格

Service Mesh 这个服务网络专注于处理服务和服务间的通讯。其主要负责构造一个稳定可靠的服务通讯的基础设施，并让整个架构更为的先进和 Cloud Native。在工程中，Service Mesh 基本来说是一组轻量级的服务代理和应用逻辑的服务在一起，并且对于应用服务是透明的。

Service Mesh 是一个基础设施。Service Mesh 是一个轻量的服务通讯的网络代理。Service Mesh 对于应用服务来说是透明无侵入的。Service Mesh 用于解耦和分离分布式系统架构中控制层面上的东西。

Service Mesh 开源软件是 Istio 和 Linkerd，它们都可以在 Kubernetes 中集成。当然，还有一个新成员 Conduit，它是由 Linkerd 的作者出来自己搞的，由 Rust 和 Go 写成的。Rust 负责数据层面，Go 负责控制面。号称吸取了很多 Linkerd 的 Scala 的教训，比 Linkerd 更快，还轻，更简单。

首先，边车模式进化的下一阶段，就是把它的功能标准化成一个集群，其结果就是服务网格。它在分布式系统中的地位，类似于七层网络模型中的传输层协议，而服务本身则只需要关心业务逻辑，因此类似于应用层协议。然后，我介绍了几个实现了服务网格的开源软件。

## 网关模式

我们讲了 Sidecar 和 Service Mesh 这两种设计模式，它们都是在不侵入业务逻辑的情况下，把控制面（control plane）和数据面（data plane）的处理解耦分离。但是这两种模式都让我们的运维成本变得特别大，因为每个服务都需要一个 Sidecar，这让本来就复杂的分布式系统的架构就更为复杂和难以管理了。

在谈 Service Mesh 的时候，我们提到了 Gateway。我个人觉得并不需要为每个服务的实例都配置上一个 Sidecar。其实，**一个服务集群配上一个 Gateway 就可以了，或是一组类似的服务配置上一个 Gateway。**
这样一来，Gateway 方式下的架构，可以细到为每一个服务的实例配置一个自己的 Gateway，也可以粗到为一组服务配置一个，甚至可以粗到为整个架构配置一个接入的 Gateway。于是，整个系统架构的复杂度就会变得简单可控起来。
![i70pwX](http://qiniu.chalme.top/blog/20230127/i70pwX.jpg)

### 网关模式设计

非常类似 阿里 Aserver.
功能：

1. 请求路由。
2. 服务注册。
3. 负载均衡。
4. 弹力设计。
5. 安全方面。SSL 加密及证书管理、Session 验证、授权、数据校验，以及对请求源进行恶意攻击的防范。

设计重点:

1. 高性能
2. 高可用
   1. 集群化。 自己可以同步信息。
   2. 服务化。网关还需要做到在不间断的情况下修改配置，一种是像 Nginx reload 配置那样，可以做到不停服务，另一种是最好做到服务化。也就是说，得要有自己的 Admin API 来在运行时修改自己的配置。
   3. 持续化。比如重启，就是像 Nginx 那样优雅地重启。有一个主管请求分发的主进程。当我们需要重启时，新的请求被分配到新的进程中，而老的进程处理完正在处理的请求后就退出。
3. 高扩展。Nginx 那样通过 Module；像 AWS Lambda 那样的方式；

运维方面：

1. 业务松耦合，协议紧耦合。
2. 应用监视，提供分析数据。
3. 用弹力设计保护后端服务。
4. DevOps

### Gateway、Sidecar 和 Service Mesh

|      | Sidecar                                | Service Mesh                                                                            | Gateway                                                               |
| ---- | -------------------------------------- | --------------------------------------------------------------------------------------- | --------------------------------------------------------------------- |
| 用途 | Sidecar 的方式主要是用来改造已有服务。 | Sidecar 在架构中越来越多时，需要我们对 Sidecar 进行统一的管理。像一个服务的 PaaS 平台。 | Service Mesh 的架构和部署太过于复杂，会让我们运维层面上的复杂度变大。 |
| 简易 |                                        |                                                                                         |                                                                       |

总而言之，我觉得 Gateway 的方式比 Sidecar 和 Service Mesh 更好。当然，具体问题还要具体分析。

## 部署升级策略

一般来说，有如下几种：

1. 停机部署（Big Bang / Recreate）
2. 蓝绿部署（Blue/Green /Stage）：部署好新版本后，把流量从老服务那边切过来。
3. 滚动部署（Rolling Update / Ramped）： 一点一点地升级现有的服务。
4. 灰度部署（Canary）：把一部分用户切到新版本上来，然后看一下有没有问题。如果没有问题就继续扩大升级，直到全部升级完成。
5. AB 测试（A/B Testing）：同时上线两个版本，然后做相关的比较。

总结：
部署应用有很多种方法，实际采用哪种方式取决于需求和预算。
当发布到开发或者模拟环境时，停机或者滚动部署是一个好选择，因为干净和快速。
当发布到生产环境时，滚动部署或者蓝绿部署通常是一个好选择，但新平台的主流程测试是必须的。

蓝绿部署也不错，但需要额外的资源。如果应用缺乏测试或者对软件的功能和稳定性影响缺乏信心，那么可以使用金丝雀部署或者 AB 测试发布。如果业务需要根据地理位置、语言、操作系统或者浏览器特征等参数来给一些特定的用户测试，那么可以采用 AB 测试技术。
![b6WjOQ](http://qiniu.chalme.top/blog/20230127/b6WjOQ.jpg)
