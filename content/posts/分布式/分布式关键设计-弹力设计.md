---
title: "分布式关键设计 弹力设计"
date: 2023-01-27T23:51:23+08:00
draft: false
tags: 
  - 分布式架构
  - 笔记
ShowToc: true
---

## 隔离设计

- 按服务的种类来做分离： 比如： 我们将系统分成了用户、商品、社区三个板块。

不好： 如果我们需要同时获得多个板块的数据，那么就需要调用多个服务，这会降低性能。注意，这里性能降低指的是响应时间，而不是吞吐量（相反，在这种架构下，吞吐量可以得到提高）。
一般： 很多： 这样的系统通常会引入大量的异步处理模型。

- 按用户的请求来做分离
  - 多租户 ： 我们将用户分成不同的组，并把后端的同一个服务根据这些不同的组分成不同的实例。
    - 完全独立的设计。每个租户有自己完全独立的服务和数据。
    - 独立的数据分区，共享的服务。多租户的服务是共享的，但数据是分开隔离的。
    - 共享的服务，共享的数据分区。每个租户的数据和服务都是共享的。
    - 分析

如果使用完全独立的方案，在开发实现上和资源隔离度方面会非常好，然而，成本会比较高，计算资源也会有一定的浪费。如果使用完全共享的方案，在资源利用和成本上会非常好，然而，开发难度非常大，而且数据和资源隔离非常不好。

### 隔离设计的重点

1. 我们需要定义好隔离业务的大小和粒度，过大和过小都不好。这需要认真地做业务上的需求和系统分析。
2. 无论是做系统板块还是多租户的隔离，你都需要考虑系统的复杂度、成本、性能、资源使用的问题，找到一个合适的均衡方案，或是分布实施的方案尤其重要，这其中需要你定义好要什么和不要什么。因为，我们不可能做出一个什么都能满足的系统。
3. 隔离模式需要配置一些高可用、重试、异步、消息中间件，流控、熔断等设计模式的方式配套使用。
4. 不要忘记了分布式系统中的运维的复杂度的提升，要能驾驭得好的话，还需要很多自动化运维的工具，尤其是使用像容器或是虚拟机这样的虚拟化技术可以帮助我们更方便地管理，和对比资源更好地利用。否则做出来了也管理不好。
5. 最后，你需要一个非常完整的能够看得到所有服务的监控系统，这点非常重要。

## 异步通讯设计

同步调用虽然让系统间只耦合于接口，而且实时性也会比异步调用要高，但是我们也需要知道同步调用会带来如下几个问题。

1. 同步调用需要被调用方的吞吐不低于调用方的吞吐。
2. 同步调用会导致调用方一直在等待被调用方完成，如果一层接一层地同步调用下去，所有的参与方会有相同的等待时间。这会非常消耗调用方的资源。因为调用方需要保存现场（Context）等待远端返回，所以对于并发比较高的场景来说，这样的等待可能会极度消耗资源。
3. 同步调用只能是一对一的，很难做到一对多。
4. 同步调用最不好的是，如果被调用方有问题，那么其调用方就会跟着出问题，于是会出现多米诺骨牌效应，故障一下就蔓延开来。

### 异步通讯的三种方式

1. 请求响应式。 在这种情况下，发送方（sender）会直接请求接收方（receiver），被请求方接收到请求后，直接返回——收到请求，正在处理。对于返回结果，有两种方法，一种是发送方时不时地去轮询一下，问一下干没干完。另一种方式是发送方注册一个回调方法，也就是接收方处理完后回调请求方。
2. 通过订阅的方式。 这种情况下，接收方（receiver）会来订阅发送方（sender）的消息，发送方会把相关的消息或数据放到接收方所订阅的队列中，而接收方会从队列中获取数据。
3. **通过 Broker 的机制。** 所谓 Broker，就是一个中间人，发送方（sender）和接收方（receiver）都互相看不到对方，它们看得到的是一个 Broker，发送方向 Broker 发送消息，接收方向 Broker 订阅消息。

### 事件驱动设计

事件驱动最好是使用 Broker 方式，服务间通过交换消息来完成交流和整个流程的驱动。
事件驱动方式的好处

- 服务间的依赖没有了，服务间是平等的，每个服务都是高度可重用并可被替换的。
- 服务的开发、测试、运维，以及故障处理都是高度隔离的。
- 服务间通过事件关联，所以服务间是不会相互 block 的。
- 在服务间增加一些 Adapter（如日志、认证、版本、限流、降级、熔断等）相当容易。
- 服务间的吞吐也被解开了，各个服务可以按照自己的处理速度处理。

坏处：

- 业务流程不再那么明显和好管理。整个架构变得比较复杂。解决这个问题需要有一些可视化的工具来呈现整体业务流程。
- 事件可能会乱序。这会带来非常 Bug 的事。解决这个问题需要很好地管理一个状态机的控制。
- 事务处理变得复杂。需要使用两阶段提交来做强一致性，或是退缩到最终一致性。

## 幂等性设计

订单创建接口，第一次调用超时了，然后调用方重试了一次。是否会多创建一笔订单？

两种处理方式。

1. 超时下游系统提供相应的查询接口。
2. 通过幂等性的方式。

对于第一种方式，需要对方提供一个查询接口来做配合。而第二种方式则需要下游的系统提供支持幂等性的交易接口。

### 唯一 ID

绍一个 Twitter 的开源项目 Snowflake
![ERMOYm](http://qiniu.chalme.top/blog/20230127/ERMOYm.jpg)

### 具体的方式

对于幂等性的处理流程来说，说白了就是要过滤一下已经收到的交易。要做到这个事，我们需要一个存储来记录收到的交易。

于是，当收到交易请求的时候，我们就会到这个存储中去查询。如果查找到了，那么就不再做查询了，并把上次做的结果返回。如果没有查到，那么我们就记录下来。

但是，上面这个流程有个问题。因为绝大多数请求应该都不会是重新发过来的，所以让 100% 的请求都到这个存储里去查一下，这会导致处理流程变得很慢。所以，最好是当这个存储出现冲突的时候会报错。也就是说，我们收到交易请求后，直接去存储里记录这个 ID（相对于数据的 Insert 操作），如果出现 ID 冲突了的异常，那么我们就知道这个之前已经有人发过来了，所以就不用再做了。比如，数据库中你可以使用 insert into … values … on DUPLICATE KEY UPDATE … 这样的操作。

对于更新的场景来说，如果只是状态更新，可以使用如下的方式。如果出错，要么是非法操作，要么是已被更新，要么是状态不对，总之多次调用是不会有副作用的。update table set status = “paid” where id = xxx and status = “unpaid”;

![zNd2vK](http://qiniu.chalme.top/blog/20230127/zNd2vK.jpg)

## 服务状态

所谓“状态”，就是为了保留程序的一些数据或是上下文。

程序调用的结果。
服务组合下的上下文。
服务的配置。

### 无状态的服务 Stateless

为了做出无状态的服务，我们通常需要把状态保存到其他的地方。
比如，不太重要的数据可以放到 Redis 中，重要的数据可以放到 MySQL 中，或是像 ZooKeeper/Etcd 这样的高可用的强一致性的存储中，或是分布式文件系统中。
于是，我们为了做成无状态的服务，会导致这些服务需要耦合第三方有状态的存储服务。一方面是有依赖，另一方面也增加了网络开销，导致服务的响应时间也会变慢。所以，第三方的这些存储服务也必须要做成高可用高扩展的方式。
而且，为了减少网络开销，还需要在无状态的服务中增加缓存机制。然而，下次这个用户的请求并不一定会在同一台机器，所以，这个缓存会在所有的机器上都创建，也算是一种浪费吧。

### 有状态的服务 Stateful

## 补偿事务

分布式系统有一个比较明显的问题就是，一个业务流程需要组合一组服务。这样的事情在微服务下就更为明显了，因为这需要业务上一致性的保证。也就是说，如果一个步骤失败了，那么要么回滚到以前的服务调用，要么不断重试保证所有的步骤都成功。

### ACID 和 BASE

传统关系型数据库系统的事务都有 ACID 属性，即原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation，又称独立性）、持久性（Durability）。
Basic Availability：基本可用。这意味着，系统可以出现暂时不可用的状态，而后面会快速恢复。Soft-state：软状态。它是我们前面的“有状态”和“无状态”的服务的一种中间状态。也就是说，为了提高性能，我们可以让服务暂时保存一些状态或数据，这些状态和数据不是强一致性的。Eventual Consistency：最终一致性，系统在一个短暂的时间段内是不一致的，但最终整个系统看到的数据是一致的。

BASE 系统是允许或是容忍系统出现暂时性问题的，这样一来，我们的系统就能更有弹力。因为我们知道，**在分布式系统的世界里，故障是不可避免的，我们能做的就是把故障处理当成功能写入代码中，这就是 Design for Failure。**

**买书：**
ACID 的玩法就是，大家在买同一本书的过程中，每个用户的购买请求都需要把库存锁住，等减完库存后，把锁释放出来，后续的人才能进行购买。于是，在 ACID 的玩法下，我们在同一时间不可能有多个用户下单，我们的订单流程需要有排队的情况，这样一来，我们就不可能做出性能比较高的系统来。

1. 用户库存， 真实库存 。
2. 亚马逊的方式， 要根据用户的地址去不同的仓库查看库存，很耗时。 异步邮件发送。

### 业务补偿

我们知道，在很多情况下，我们是无法做到强一致的 ACID 的。特别是我们需要跨多个系统的时候，而且这些系统还不是由一个公司所提供的。

一般来说，业务的事务补偿都是需要一个工作流引擎的。亚马逊是一个超级喜欢工作流引擎的公司，这个工作流引擎把各式各样的服务给串联在一起，并在工作流上做相应的业务补偿，整个过程设计成为最终一致性的。

一个好的业务补偿机制需要做到下面这几点。

1. 要能清楚地描述出要达到什么样的状态（比如：请假、机票、酒店这三个都必须成功，租车是可选的），以及如果其中的条件不满足，那么，我们要回退到哪一个状态。这就是所谓的整个业务的起始状态定义。
2. 当整条业务跑起来的时候，我们可以串行或并行地做这些事。对于旅游订票是可以并行的，但是对于网购流程（下单、支付、送货）是不能并行的。总之，我们的系统需要努力地通过一系列的操作达到一个我们想要的状态。如果达不到，就需要通过补偿机制回滚到之前的状态。这就是所谓的状态拟合。
3. 对于已经完成的事务进行整体修改，可以考虑成一个修改事务。

### 业务补偿的设计重点

业务补偿主要做两件事。

- 努力地把一个业务流程执行完成。
- 如果执行不下去，需要启动补偿机制，回滚业务流程。

1. 把一个业务流程执行完成，需要这个流程中所涉及的服务方支持幂等性。并且在上游有重试机制。
2. 我们需要小心维护和监控整个过程的状态，所以，千万不要把这些状态放到不同的组件中，最好是一个业务流程的控制方来做这个事，也就是一个工作流引擎。
3. 补偿的业务逻辑和流程不一定非得是严格反向操作。有时候可以并行，有时候可以串行，可能会更简单。
4. 业务补偿的业务逻辑是强业务相关的，很难做成通用的。

## 重试

### 场景

“重试”的语义是我们认为这个故障是暂时的，而不是永久的，所以，我们会去重试。
设计重试时，我们需要**定义出什么情况下需要重试**，例如，调用超时、被调用端返回了某种可以重试的错误（如繁忙中、流控中、维护中、资源不足等）。
一些别的错误，则最好不要重试，比如：业务级的错误（如没有权限、或是非法数据等错误），技术上的错误（如：HTTP 的 503 等，这种原因可能是触发了代码的 bug，重试下去没有意义）。

### 策略

1. 都需要有个重试的最大值，经过一段时间不断的重试后，就没有必要再重试了，应该报故障了。
2. ，Exponential Backoff 的策略，也就是所谓的“指数级退避”。（TCP 拥塞控制）
3. Spring 的重试策略。个单独实现重试功能的项目

### 设计的重点

1. 要确定什么样的错误下需要重试；
2. 重试的时间和重试的次数。 不同场景可能需要不同的策略。 （比如流控，那么应该使用指数退避的方式，以避免造成更多的流量。）
3. 如果超过重试次数，或是一段时间，那么重试就没有意义了。
4. 重试还需要考虑被调用方是否有幂等的设计。
5. 对于有事务相关的操作。我们可能会希望能重试成功，而不至于走业务补偿那样的复杂的回退流程。 记录请求。

## 熔断设计

熔断机制这个词对你来说肯定不陌生，它的灵感来源于我们电闸上的“保险丝”。
在我们的分布式系统设计中，也应该有这样的方式。**前面说过重试机制，如果错误太多，或是在短时间内得不到修复，那么我们重试也没有意义了，此时应该开启我们的熔断操作，尤其是后端太忙的时候，使用熔断设计可以保护后端不会过载。**
熔断设计是受了电路设计中保险丝的启发，其需要实现三个状态：闭合、断开和半开，分别对应于正常、故障和故障后检测故障是否已被修复的场景，并介绍了 Netflix 的 Hystrix 对熔断的实现。

## 限流设计

目的： **保护系统不会在过载的情况下出现问题，我们就需要限流。**

### 限流策略

- 拒绝服务。把多出来的请求拒绝掉
- 服务降级。关闭或是把后端服务做降级处理。
- 特权请求。 vip
- 延时处理。 短暂的峰值。
- 弹性伸缩。 数据库压力过大，没用。

### 限流的实现方式

计数器方式，

队列算法（优先级），漏桶法，令牌桶法。 基本原理都是 队列缓存。
固定窗口，滑动窗口： (sentinel)
[https://ata.alibaba-inc.com/articles/249558](https://ata.alibaba-inc.com/articles/249558)
环形数组 LeapArray 来统计实时的秒级指标数据. 写多于读的高并发场景。
环形数组：

1. 构建 数组长度， 数据的时间 。 每个窗口时间
2. 定位： 时间戳/ 窗口时间 % 数据长度 。 窗口开始 = 时间戳 - 时间戳%时间间隔
3. 判断当前窗口 ？
   1. 为空，直接创建新的窗口， cas
   2. 不为空， old 窗口时间起始时间 = 当前 ， 直接返回；
   3. 部位空， old 窗口时间起始时间 ！= 当前， reset 当前窗口。

**根据响应时间自动限流：**
就是需要设置一个确定的限流值。这就要求我们每次发布服务时都做相应的性能测试，找到系统最大的性能值。
很多时候， 很难给出一个合适的值。

1. 实际情况下，很多服务会依赖于数据库。 数据类型， 数据量。
   1. 所以，不同的用户请求，会对不同的数据集进行操作。就算是相同的请求，可能数据集也不一样，比如，现在很多应用都会有一个时间线 Feed 流，不同的用户关心的主题人人不一样，数据也不一样。
2. 不同的 API 有不同的性能。我们要在线上为每一个 API 配置不同的限流值，这点太难配置，也很难管理。
3. 而且，现在的服务都是能自动化伸缩的，不同大小的集群的性能也不一样，所以，在自动化伸缩的情况下，我们要动态地调整限流的阈值，这点太难做到了。

如何设计？ 如何做？
不再设定一个特定的流控值，而是能够动态地感知系统的压力来自动化地限流。 典范是 TCP 协议的拥塞控制的算法。
**具体：**
我们记录下每次调用后端请求的响应时间，然后在一个时间区间内（比如，过去 10 秒）的请求计算一个响应时间的 P90 或 P99 值，也就是把过去 10 秒内的请求的响应时间排个序，然后看 90% 或 99% 的位置是多少。

P90 或 P99 值： 消耗内存和 CPU。 1) 采样 。
动态流控需要像 TCP 那样，你需要记录一个当前的 QPS。如果发现后端的 P90/P99 响应太慢，那么就可以把这个 QPS 减半，然后像 TCP 一样走慢启动的方式，直接到又开始变慢，然后减去 1/4 的 QPS，再慢启动，然后再减去 1/8 的 QPS。

### 限流的设计要点

**目的：**

1. 为了向用户承诺 SLA。
2. 多租户的情况下，某一用户把资源耗尽。
3. 应对突发的流量
4. 节约成本。我们不会为了一个不常见的尖峰来把我们的系统扩容到最大的尺寸。

**设计上，我们还要有以下的考量。**

1. 限流模块性能必须好，而且对流量的变化也是非常灵敏的，否则太过迟钝的限流，系统早因为过载而挂掉了。
2. 限流应该有个手动的开关，这样在应急的时候，可以手动操作。
3. 当限流发生时，应该有个监控事件通知。 运维跟进。 自动扩容。
4. 当限流发生时，对于拒掉的请求，我们应该返回一个特定的限流错误码。 看到这些错误码， 调整发送速度，或走重试机制。
5. 限流应该让后端的服务感知到。 比如： 在协议头中塞进一个标识 。 看到标识， 可以降级。

## 降级

所谓的降级设计（Degradation），本质是为了解决资源不足和访问量过大的问题。**暂时牺牲掉一些东西，以保障整个系统的平稳运行。**

**一般的降级：**

1. 降低一致性。从强一致性变成最终一致性。
2. 停止次要功能。停止访问不重要的功能，从而释放出更多的资源。
3. 简化功能。把一些功能简化掉，比如，简化业务流程，或是不再返回全量数据，只返回部分数据。

我们现在的降级分为功能降级和服务降级。我们目前将降级开关放到配置中心。
一、功能降级
1、通过降级开关控制功能可用不可用，一般为页面和按钮
2、简化业务操作流程，当降级后简化业务操作步骤，快速完成业务操作
二、服务降级
1、读降级，降级前会读缓存，缓存中不存在的话读数据库，降级后读缓存，缓存中不存在的话，返回默认值，不再读数据库
2、写降级，将之前的同步写数据库降级为先写缓存，然后异步写库
3、服务调用降级，之前两个系统模块通过 mq 来交互，当 mq 消息积压或 mq 宕机出问题后，降级为服务直接调用

## 总结

### 弹力设计总图

**服务不是单点**。

1. 负载均衡 + 服务健康检查。 （nginx 或 haproxy)
2. 服务发现 + 动态路由 + 服务健康检查。 (zookeeper + consul)
3. 自动化运维，Kubernetes 服务调度、伸缩和故障迁移.

**隔离我们的业务**，要隔离我们的服务我们就需要对服务进行解耦和拆分

1. bulkheads 模式：业务分片 、用户分片、数据库拆分。
2. 自包含系统：所谓自包含的系统是从单体到微服务的中间状态，其把一组密切相关的微服务给拆分出来，只需要做到没有外部依赖就行。
3. 异步通讯：服务发现、事件驱动、消息队列、业务工作流。
4. 自动化运维：需要一个服务调用链和性能监控的监控系统。

接受失败的相关处理设计，也就是所谓的**容错设计**

1. 错误方面：调用重试 + 熔断 + 服务的幂等性设计。
2. 一致性方面：强一致性使用两阶段提交、最终一致性使用异步通讯方式。
3. 流控方面：使用限流 + 降级技术。
4. 自动化运维方面：网关流量调度，服务监控。

![BXIC8q](http://qiniu.chalme.top/blog/20230127/BXIC8q.jpg)
有三大块的东西

- 冗余服务。通过冗余服务的复本数可以消除单点故障。
- 服务解耦。通过解耦可以做到把业务隔离开来，不让服务受影响，这样就可以有更好的稳定性。
- 服务容错。服务容错方面，需要有重试机制，重试机制会带来幂等操作，对于服务保护来说，熔断，限流，降级都是为了保护整个系统的稳定性，并在可用性和一致性方面在出错的情况下做一部分的妥协。

当然，除了这一切的架构设计外，你还需要一个或多个自动运维的工具。否则，如果是人肉运维的话，那么在故障发生的时候，不能及时地做出运维决定，也就空有这些弹力设计了。比如：**监控到服务性能不够了，就自动或半自动地开始进行限流或降级。**
